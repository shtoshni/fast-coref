{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from os import path\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0211 13:18:27.436591 139886178457408 tokenization_utils.py:373] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/shtoshni/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_START = '<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"></head>'\n",
    "\n",
    "cluster_start_tag = '<div class=\"{}\" style=\"border:2px; display : inline; border-style:solid; padding: {}px; border-color: {}; padding-right: 3px; padding-left: 3px\">'\n",
    "singleton_start_tag = '<div class=\"{}\" style=\"border:2px; display : inline; border-style:dotted; padding:{}px; border-color: {}; padding-right: 3px; padding-left: 3px\">'\n",
    "end_tag = '</div>'\n",
    "\n",
    "script_string =(\n",
    "    \"\"\"\n",
    "    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js\"></script>\n",
    "<script>\n",
    "$(document).ready(function(){\n",
    "  $(\"div\").mouseover(function(e){\n",
    "      if( $(this).attr('class').match('cluster*') ) {\n",
    "        $(document.getElementsByClassName($(e.target).attr('class'))).css(\"background-color\", \"#C0F3F9\");\n",
    "    }\n",
    "  //}\n",
    "  //}\n",
    "    //  , function(e){\n",
    "      //  $(document.getElementsByClassName($(e.target).attr('class'))).css(\"background-color\", \"#fff\");\n",
    "    });\n",
    "\n",
    "  $(\"div\").mouseout(function(e){\n",
    "      if( $(this).attr('class').match('cluster*') ) {\n",
    "        $(document.getElementsByClassName($(e.target).attr('class'))).css(\"background-color\", \"#fff\");\n",
    "    }\n",
    "  //}\n",
    "  //}\n",
    "    //  , function(e){\n",
    "      //  $(document.getElementsByClassName($(e.target).attr('class'))).css(\"background-color\", \"#fff\");\n",
    "    });\n",
    "\n",
    "});\n",
    "</script>\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "largest_padding = 13\n",
    "padding_reduction = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_html(json_file):\n",
    "    story_name = path.basename(story_file)\n",
    "    print(story_name)\n",
    "    \n",
    "    with open(json_file) as f:\n",
    "        doc = json.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    body_string ='<body><div class=\"wrapper\" style=\"line-height: 3\">'\n",
    "    \n",
    "    all_subtokens = []\n",
    "    for sentence in doc[\"tokenized_doc\"][\"sentences\"]:\n",
    "        all_subtokens.extend(sentence)\n",
    "    \n",
    "    all_tokens = []\n",
    "    cur_subtoken_list = []\n",
    "    cur_idx = 0\n",
    "    for idx, subtoken_idx in enumerate(doc[\"tokenized_doc\"][\"subtoken_map\"]):\n",
    "        if subtoken_idx == cur_idx:\n",
    "            if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "                cur_subtoken_list.append(all_subtokens[idx])\n",
    "        else:\n",
    "#             print(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "            all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "            if all_subtokens[idx] not in ['[SEP]', '[CLS]']:\n",
    "                cur_subtoken_list = [all_subtokens[idx]]\n",
    "            else:\n",
    "                cur_subtoken_list = []\n",
    "            cur_idx = subtoken_idx\n",
    "            \n",
    "    \n",
    "#     print(all_tokens[:30])\n",
    "    if cur_subtoken_list != []:\n",
    "        all_tokens.append(bert_tokenizer.convert_tokens_to_string(cur_subtoken_list))\n",
    "    \n",
    "#     print(len(doc[\"tokenized_doc\"][\"subtoken_map\"]))\n",
    "    print(len(all_tokens), doc[\"tokenized_doc\"][\"subtoken_map\"][-1] + 1)\n",
    "#     print(len(all_subtokens))\n",
    "    \n",
    "    ment_start_dict = defaultdict(list)\n",
    "    ment_end_dict = defaultdict(list)\n",
    "    cluster_idx_to_len = defaultdict(int)\n",
    "    \n",
    "    clusters = sorted(doc[\"clusters\"], key=lambda x: x[0][0][0])\n",
    "    \n",
    "    for cluster_idx, ment_list in enumerate(clusters):\n",
    "        ment_list  = sorted(ment_list, key=lambda x: x[0][0])\n",
    "        for (ment_start, ment_end), string in ment_list:\n",
    "            ment_start_dict[ment_start].append((ment_end, cluster_idx))\n",
    "            ment_end_dict[ment_end].append((ment_start, cluster_idx))\n",
    "            cluster_idx_to_len[cluster_idx] += 1\n",
    "                        \n",
    "    # Sort mentions with same mention start by later mention ends i.e. start with spans which are longer\n",
    "    for ment_start in ment_start_dict:\n",
    "        ment_start_dict[ment_start] = sorted(ment_start_dict[ment_start], key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    # Sort mentions with same mention end by later mention starts i.e. start with spans which are shorter\n",
    "    for ment_end in ment_end_dict:\n",
    "        ment_end_dict[ment_end] = sorted(ment_end_dict[ment_end], key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    active_clusters = 0\n",
    "    cluster_seen = set()\n",
    "    for token_idx, token in enumerate(all_tokens):\n",
    "        token_added = False\n",
    "        if token == \"\\n\":\n",
    "            body_string += \"<br/>\\n\" \n",
    "            continue\n",
    "        if token_idx in ment_start_dict:\n",
    "            for (_, cluster_idx) in ment_start_dict[token_idx]:\n",
    "                prefix = cluster_start_tag\n",
    "                cluster_name = f\"cluster_{str(cluster_idx).zfill(4)}\"\n",
    "                if cluster_idx_to_len[cluster_idx] == 1:\n",
    "                    prefix = singleton_start_tag\n",
    "                if cluster_idx in cluster_seen:\n",
    "                    color = '#000000'\n",
    "                else:\n",
    "                    color = '#F64A8A'\n",
    "                \n",
    "                \n",
    "                cluster_seen.add(cluster_idx)\n",
    "                body_string += prefix.format(cluster_name, \n",
    "                    largest_padding - active_clusters * padding_reduction, color)\n",
    "                active_clusters += 1\n",
    "            \n",
    "            body_string += token + \" \"\n",
    "            token_added = True\n",
    "        \n",
    "        if not token_added:\n",
    "            body_string += token + \" \"\n",
    "\n",
    "        if token_idx in ment_end_dict:\n",
    "            for (_, cluster_idx) in ment_end_dict[token_idx]:\n",
    "                body_string += \"<sub>\" + str(cluster_idx).zfill(4) + \"</sub>\" + end_tag + \" \"\n",
    "                active_clusters -= 1\n",
    "                assert (active_clusters >= 0)\n",
    "    \n",
    "    html_string = HTML_START + script_string + body_string + \"</div></body></html>\"\n",
    "    return html_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all files to get HTML version of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umem_ccarol_output.json\n",
      "41698 41698\n"
     ]
    }
   ],
   "source": [
    "story_file = \"/home/shtoshni/Research/long-doc-coref/notebooks/umem_ccarol_output.json\"\n",
    "book_html = return_html(story_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shtoshni/Research/long-doc-coref/notebooks/umem_ccarol_output.html\n"
     ]
    }
   ],
   "source": [
    "html_file = path.join(\"/home/shtoshni/Research/long-doc-coref/notebooks/\", \n",
    "                      path.splitext(path.basename(story_file))[0] + \".html\")\n",
    "\n",
    "print(html_file)\n",
    "with open(html_file, \"w\") as f:\n",
    "    f.write(book_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:narrative_10]",
   "language": "python",
   "name": "conda-env-narrative_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
