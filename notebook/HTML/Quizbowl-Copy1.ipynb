{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import json\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from transformers import LongformerTokenizerFast\n",
    "\n",
    "sys.path.append(\"/home/shtoshni/Research/fast-coref/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"/home/shtoshni/Research/fast-coref/models/joint_downsample_30K/character_identification/test.log.jsonl\"\n",
    "base_output_dir = \"/home/shtoshni/Research/fast-coref/models/html/coref\"\n",
    "\n",
    "model_name = path.basename(path.dirname(input_file))\n",
    "\n",
    "output_dir = path.join(base_output_dir, \"character_identification/joint\")\n",
    "if not path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-large-4096', add_prefix_space=True)\n",
    "\n",
    "SPEAKER_START = '[SPEAKER_START]'\n",
    "SPEAKER_END = '[SPEAKER_END]'\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "            'additional_special_tokens': [SPEAKER_START, SPEAKER_END]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_START = '<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"></head><body>'\n",
    "\n",
    "\n",
    "start_tag_template = '<div style=\"border:2px; display:inline; border-style: {}; border-color: {}; padding: {}px; padding-right: 3px; padding-left: 3px\">'\n",
    "end_tag = '</div>'\n",
    "\n",
    "largest_padding = 14\n",
    "padding_reduction = 3\n",
    "\n",
    "\n",
    "def get_tag_options(cluster_len, tag_type=\"gt\"):\n",
    "    if cluster_len > 1:\n",
    "        border = 'solid'\n",
    "    else:\n",
    "        border = 'dotted'\n",
    "    color = '#FFD700'\n",
    "    if tag_type == \"pred\":\n",
    "        color = 'red'\n",
    "        \n",
    "    return (border, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mentions(clusters):\n",
    "    mentions = set()\n",
    "    mention_spans = set()\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for mention in cluster:\n",
    "            span_start, span_end = mention\n",
    "            mentions.add((span_start, span_end))\n",
    "            mention_spans.add((span_start, span_end))\n",
    "            \n",
    "    return mentions, mention_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_files = []\n",
    "\n",
    "with open(input_file) as f:\n",
    "    for line in f:\n",
    "        instance = json.loads(line.strip())\n",
    "\n",
    "        bert_seg_idx = []\n",
    "        doc_list = [] \n",
    "        for sentence in instance[\"sentences\"]:\n",
    "            doc_list.extend(sentence)\n",
    "            bert_seg_idx.append(len(sentence) + (bert_seg_idx[-1] if len(bert_seg_idx) else 0))\n",
    "\n",
    "        bert_seg_idx = set(bert_seg_idx)\n",
    "        html_tag_list = {}\n",
    "\n",
    "        # Get all the entity info\n",
    "        clusters = sorted(instance[\"clusters\"], key=lambda cluster: min([elem[0] for elem in cluster]))\n",
    "        pred_clusters = sorted(instance[\"predicted_clusters\"], key=lambda cluster: min([elem[0] for elem in cluster]))\n",
    "        \n",
    "        all_mentions, all_spans = get_mentions(clusters)\n",
    "        all_pred_mentions, all_pred_spans = get_mentions(pred_clusters)\n",
    "        \n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            for mention in cluster:\n",
    "                span_start, span_end = mention\n",
    "                \n",
    "                corr = '' \n",
    "                if (span_start, span_end) not in all_pred_mentions:\n",
    "                    corr = 'MISSED '\n",
    "                    if (span_start, span_end) in all_pred_spans:\n",
    "                        corr = ''\n",
    "\n",
    "                span_end = span_end + 1  ## Now span_end is not part of the span\n",
    "                ent_type = f'{corr}' #, {mention_info[\"realis\"]}'\n",
    "                       \n",
    "\n",
    "                if span_start not in html_tag_list:\n",
    "                    html_tag_list[span_start] = defaultdict(list)\n",
    "                if span_end not in html_tag_list:\n",
    "                    html_tag_list[span_end] = defaultdict(list)\n",
    "\n",
    "                subscript = ent_type + \" \" + str(cluster_idx)\n",
    "\n",
    "                tag_options = get_tag_options(len(cluster))\n",
    "                start_tag = start_tag_template.format(\n",
    "                    *tag_options, \n",
    "                    largest_padding - padding_reduction * len(html_tag_list[span_start]['start']))\n",
    "\n",
    "\n",
    "                html_tag_list[span_start]['start'].append((start_tag, span_end))\n",
    "                # Subscript used in end\n",
    "                html_tag_list[span_end]['end'].append((span_start, cluster_idx, end_tag, subscript))\n",
    "                \n",
    "        \n",
    "        # Get all the entity info\n",
    "        \n",
    "        for cluster_idx, cluster in enumerate(pred_clusters):\n",
    "            for mention in cluster:\n",
    "                span_start, span_end = mention\n",
    "                corr = ''\n",
    "                if (span_start, span_end) not in all_mentions:\n",
    "                    corr = 'WRONG '\n",
    "                    if (span_start, span_end) not in all_spans:\n",
    "                        corr = 'EXTRA '\n",
    "                        \n",
    "                \n",
    "                span_end = span_end + 1  ## Now span_end is not part of the span\n",
    "                \n",
    "                ent_type = f'{corr}'\n",
    "\n",
    "                if span_start not in html_tag_list:\n",
    "                    html_tag_list[span_start] = defaultdict(list)\n",
    "                if span_end not in html_tag_list:\n",
    "                    html_tag_list[span_end] = defaultdict(list)\n",
    "\n",
    "                subscript = ent_type + \" \" + str(cluster_idx)\n",
    "\n",
    "                tag_options = get_tag_options(len(cluster), \"pred\")\n",
    "                start_tag = start_tag_template.format(\n",
    "                    *tag_options, \n",
    "                    largest_padding - padding_reduction * len(html_tag_list[span_start]['start']))\n",
    "\n",
    "\n",
    "                html_tag_list[span_start]['start'].append((start_tag, span_end))\n",
    "                # Subscript used in end\n",
    "                html_tag_list[span_end]['end'].append((span_start, cluster_idx, end_tag, subscript))\n",
    "\n",
    "\n",
    "        html_string = HTML_START + '<div style=\"line-height: 3\">'\n",
    "        for token_idx, token in enumerate(doc_list):\n",
    "            \n",
    "            if token_idx in bert_seg_idx:\n",
    "                html_string += \"\\n<br/>\"\n",
    "\n",
    "            if token_idx in html_tag_list:\n",
    "                for tag_type in ['end', 'start']:\n",
    "                    if tag_type == 'end' and (tag_type in html_tag_list[token_idx]):\n",
    "                        tags = html_tag_list[token_idx]['end']\n",
    "\n",
    "                        # Sort the tags so as to mimic the stack behavior\n",
    "                        tags = tags[::-1]  # Highest mentions first\n",
    "                        for _, _, html_tag, subscript in tags:\n",
    "                            html_string += \"<sub>\" + subscript + \"</sub>\" \n",
    "                            html_string += html_tag\n",
    "                            # Since we are deleting the highest indices first, the lower indices are unaffected\n",
    "\n",
    "                    if tag_type == 'start' and (tag_type in html_tag_list[token_idx]):\n",
    "                        sorted_tag_list = sorted(html_tag_list[token_idx]['start'], key=lambda x: x[1], reverse=True)\n",
    "                        for html_tag, _ in sorted_tag_list:\n",
    "                            html_string += html_tag\n",
    "            \n",
    "            token_string = tokenizer.convert_ids_to_tokens(token).strip()\n",
    "            if token_string[0] == \"Ä \":\n",
    "                token_string = token_string[1:]\n",
    "            html_string += \" \" + token_string\n",
    "\n",
    "        html_string += \"</div></body></html>\"\n",
    "        html_string = html_string.replace(\"\\n\", \"\\n<br/>\")\n",
    "        html_string = html_string.replace(\"~\", \"&lt;\")\n",
    "        html_string = html_string.replace(\"^\", \"&gt;\")\n",
    "\n",
    "        file_name = instance[\"doc_key\"].replace(\"/\", \"-\") + \".html\"\n",
    "#         print(file_name)\n",
    "        file_path = path.join(output_dir, file_name)\n",
    "        html_files.append(file_name)\n",
    "        with open(file_path, \"w\") as f:\n",
    "            f.write(html_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shtoshni/Research/fast-coref/models/html/coref/character_identification/joint/index.html\n"
     ]
    }
   ],
   "source": [
    "index_html = HTML_START + 'Tag usage:<br/> <ul type=\"1\">'\n",
    "\n",
    "index_html += '<li> Ground truth in Golden, and Pred mentions in Pink rectangles </li>\\n' \n",
    "index_html += '<li> Singletons in dotted, and chains in solid borders </li>'\n",
    "index_html += '''<li> To highlight errors, there are a few text markers: \n",
    "(a) MISSED - mention in GT not in pred, (b) EXTRA - mention in pred not in GT, (c) WRONG - span match but wrong mention type</li>'''\n",
    "\n",
    "index_html += '</ul>\\n\\n<ol type=\"1\">'\n",
    "\n",
    "\n",
    "\n",
    "for html_file in html_files:\n",
    "    base_name = path.splitext(path.basename(html_file))[0].replace(\"-\", \"/\")\n",
    "    index_html += '<li> <a href=\"{}\", target=\"_blank\">'.format(html_file) + base_name + '</a></li>\\n'\n",
    "    \n",
    "index_html += '</ol>\\n</body>\\n</html>'\n",
    "index_file_path = path.join(output_dir, \"index.html\")\n",
    "print(index_file_path)\n",
    "with open(path.join(output_dir, \"index.html\"), \"w\") as g:\n",
    "    g.write(index_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
